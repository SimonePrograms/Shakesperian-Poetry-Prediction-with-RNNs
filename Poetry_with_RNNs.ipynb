{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <i> <center> Generating Poetic Texts with Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries needed:\n",
    "\n",
    "### 1. Random\n",
    "### 2. Tensorflow\n",
    "### 3. Sequential, LSTM, Dense, Activation, and RMSprop modules from TensorFlow\n",
    "### 4. Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.src.optimizers.rmsprop.RMSprop"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, LSTM, Activation\n",
    "# from tensorflow.keras.optimizers import RMSprop\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "Sequentialfunc = tf.keras.models.Sequential\n",
    "Densefunc = tf.keras.layers.Dense\n",
    "Activationfunc = tf.keras.layers.Activation\n",
    "LSTMfunc = tf.keras.layers.LSTM\n",
    "RMSpropfunc = tf.keras.optimizers.RMSprop\n",
    "Adamfunc = tf.keras.optimizers.Adam\n",
    "\n",
    "\n",
    "\n",
    "RMSpropfunc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "introduction\n",
      "good [morning/afternoon/evening], everyone. today, we're going to delve into the fascinating world of advanced machine learning and artificial intelligence applications. these technologies are revolutionizing numerous industries, from healthcare to finance, and from cybersecurity to art. we'll explore some of the most impactful projects and discuss the technologies, skills, and tools involved.\n",
      "\n",
      "section 1: autonomous vehicle navigation system\n",
      "let's start with autonomous vehicles, one of the most talked-about applications of ai. self-driving cars use a combination of deep learning and reinforcement learning to navigate the complex environments they encounter on the road.\n",
      "\n",
      "objective:\n",
      "develop a self-driving car navigation system using deep learning and reinforcement learning.\n",
      "\n",
      "skills:\n",
      "to undertake this project, you'll need a strong grasp of convolutional neural networks (cnns) and recurrent neural networks (rnns). cnns are essential for processing the visual data from the car's cameras, while rnns help in making sense of sequences of data, such as the car's movement over time. reinforcement learning is crucial for enabling the car to learn from its environment and improve its driving strategy.\n",
      "\n",
      "tools:\n",
      "the primary tools for this project include tensorflow or keras for building neural networks, opencv for image processing, and the robot operating system (ros) for integrating all the sensors and components. the carla simulator can be used to test and refine your algorithms in a controlled environment before deploying them on a real vehicle.\n",
      "\n",
      "section 2: ai for healthcare: disease diagnosis\n",
      "next, let's move to healthcare, where ai is making significant strides in disease diagnosis.\n",
      "\n",
      "objective:\n",
      "build a deep learning model to diagnose diseases from medical images, such as x-rays or mris.\n",
      "\n",
      "skills:\n",
      "this project requires knowledge of image classification and segmentation, as well as experience with transfer learning. transfer learning allows you to leverage pre-trained models and adapt them to your specific dataset, which can save a lot of time and computational resources. additionally, understanding explainable ai techniques is important to ensure that medical professionals can trust and interpret the ai's decisions.\n",
      "\n",
      "tools:\n",
      "you'll use tensorflow or pytorch for model development, opencv and scikit-image for image processing, and dicom libraries to handle medical image formats. ensuring your model can explain its decisions is vital in a healthcare setting, where transparency and accuracy are paramount.\n",
      "\n",
      "section 3: financial market prediction\n",
      "ai is also transforming the financial industry, particularly in predicting market trends and optimizing trading strategies.\n",
      "\n",
      "objective:\n",
      "create a machine learning model to predict stock prices and optimize trading strategies.\n",
      "\n",
      "skills:\n",
      "you'll need expertise in time series analysis and familiarity with lstm or gru networks, which are particularly suited for handling sequential data like stock prices. reinforcement learning can be employed to develop trading strategies that adapt and improve over time.\n",
      "\n",
      "tools:\n",
      "pandas and numpy are essential for data manipulation and analysis. tensorflow or keras will be used to build your models, while the scikit-learn library provides useful tools for preprocessing and evaluating your models. the ta-lib library is invaluable for technical analysis and developing trading indicators.\n",
      "\n",
      "section 4: natural language processing (nlp) for sentiment analysis\n",
      "natural language processing (nlp) is another area where ai is making a huge impact, particularly in sentiment analysis.\n",
      "\n",
      "objective:\n",
      "develop a sentiment analysis tool that can analyze social media posts and predict public sentiment on various topics.\n",
      "\n",
      "skills:\n",
      "you should be well-versed in nlp techniques and transformer models like bert or gpt. these models are state-of-the-art for understanding and generating human language. you'll also need skills in text classification and unsupervised learning to handle the vast amount of unstructured text data available online.\n",
      "\n",
      "tools:\n",
      "hugging face's transformers library is a great resource for working with pre-trained transformer models. you'll also use nltk and spacy for text preprocessing and analysis, along with tensorflow or keras for model building.\n",
      "\n",
      "section 5: recommender system for e-commerce\n",
      "e-commerce platforms heavily rely on recommender systems to personalize user experience and boost sales.\n",
      "\n",
      "objective:\n",
      "build a recommendation engine for an e-commerce platform to personalize user experience.\n",
      "\n",
      "skills:\n",
      "understanding collaborative filtering, content-based filtering, and matrix factorization is crucial. deep learning techniques are increasingly being used to enhance recommendation accuracy.\n",
      "\n",
      "tools:\n",
      "scikit-learn provides robust tools for collaborative and content-based filtering. tensorflow or keras can be used for implementing deep learning models. libraries like surprise and implicit are specifically designed for building recommendation systems.\n",
      "\n",
      "section 6: ai for smart manufacturing\n",
      "ai is also playing a crucial role in smart manufacturing, particularly in predictive maintenance and quality control.\n",
      "\n",
      "objective:\n",
      "implement predictive maintenance and quality control using machine learning models in a manufacturing setting.\n",
      "\n",
      "skills:\n",
      "you'll need skills in anomaly detection and time series forecasting. analyzing iot data is also a critical part of this project, as it involves handling large amounts of sensor data from manufacturing equipment.\n",
      "\n",
      "tools:\n",
      "pandas and numpy are essential for data manipulation. tensorflow or keras can be used to develop your models, while apache kafka and mqtt are useful for managing and processing real-time iot data streams.\n",
      "\n",
      "section 7: ai in cybersecurity\n",
      "cybersecurity is another domain where ai is making significant inroads, helping to detect and prevent cyber threats.\n",
      "\n",
      "objective:\n",
      "develop a system to detect and prevent cyber threats using machine learning models.\n",
      "\n",
      "skills:\n",
      "anomaly detection and clustering are key techniques here. both supervised and unsupervised learning methods are used to analyze network traffic and identify potential threats.\n",
      "\n",
      "tools:\n",
      "tensorflow or keras and scikit-learn will be your primary tools for developing machine learning models. wireshark is useful for network analysis, while pandas and numpy are essential for data manipulation.\n",
      "\n",
      "section 8: human activity recognition using wearable sensors\n",
      "wearable technology is becoming increasingly popular, and ai can enhance its capabilities significantly.\n",
      "\n",
      "objective:\n",
      "build a model to recognize human activities, such as walking or running, using data from wearable sensors.\n",
      "\n",
      "skills:\n",
      "signal processing and time series analysis are crucial for this project. you'll also need to develop classification models to accurately identify different activities.\n",
      "\n",
      "tools:\n",
      "pandas and numpy for data manipulation, tensorflow or keras for model building, and specialized sensor data libraries to handle the data from wearable devices.\n",
      "\n",
      "section 9: ai for drug discovery\n",
      "the pharmaceutical industry is leveraging ai to speed up the drug discovery process, potentially saving years of research and billions of dollars.\n",
      "\n",
      "objective:\n",
      "develop models to predict the efficacy of new drug compounds using chemical data.\n",
      "\n",
      "skills:\n",
      "you need a strong understanding of molecular modeling and graph neural networks. cheminformatics, the application of computer and informational techniques to solve chemical problems, is also crucial.\n",
      "\n",
      "tools:\n",
      "rdkit and deepchem are specialized libraries for cheminformatics and drug discovery. tensorflow or keras and scikit-learn are used for building and evaluating machine learning models.\n",
      "\n",
      "section 10: generative adversarial networks (gans) for art creation\n",
      "finally, let's explore the creative side of ai with generative adversarial networks (gans).\n",
      "\n",
      "objective:\n",
      "create an ai model that can generate new pieces of art.\n",
      "\n",
      "skills:\n",
      "you should be proficient in gans and have a solid understanding of deep learning techniques. creative ai is a fascinating field that combines technical skills with artistic sensibilities.\n",
      "\n",
      "tools:\n",
      "tensorflow or keras and pytorch are the primary tools for developing gans. opencv is useful for handling and processing images.\n",
      "\n",
      "conclusion\n",
      "in conclusion, the projects we've discussed today represent just a fraction of the potential applications of advanced machine learning and ai. each of these projects not only requires a deep understanding of specific technical skills and tools but also a creative and innovative mindset to apply these technologies in new and impactful ways. as you continue to explore and develop your expertise in machine learning and ai, remember that the ultimate goal is to create solutions that can positively influence the world across various industries. thank you for your attention, and i wish you all the best in your future endeavors in this exciting field.\n"
     ]
    }
   ],
   "source": [
    "# textfile = open(\"C:/Users/simon/Python/Machine_Learning/shakespeare.txt\", \"rb\").read().decode(encoding=\"utf-8\").lower()\n",
    "textfile = open(\"C:/Users/simon/Python/Machine_Learning/Script_RNN.txt\", \"rb\").read().decode(encoding=\"utf-8\").lower()\n",
    "\n",
    "print(textfile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmed = PorterStemmer()\n",
    "\n",
    "def stemming(content):\n",
    "\n",
    "    stemmed_content = re.sub(\"[^a-zA-Z]\", \" \", content) # this removes all the characters that are not alphabetical\n",
    "    stemmed_content = stemmed_content.lower() # thsi will turn allt he words into lower case\n",
    "    # stemmed_content = stemmed_content.split()\n",
    "    # stemmed_content = [stemmed.stem(word) for word in stemmed_content if not word in stopwords.words(\"english\")]\n",
    "    # stemmed_content = \" \".join(stemmed_content)\n",
    "\n",
    "    return stemmed_content\n",
    "\n",
    "\n",
    "textfile = stemming(textfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentences to numerical format to then pass a numpy array to the neural network.\n",
    "\n",
    "# textfile = textfile[10000 : 800000] # reducing file to these characters to save time of code execution.\n",
    "\n",
    "characters = sorted(set(textfile)) # set command filters out all unique characters that appear one time. While sorted command arranges every charcaters in alphabetical order.\n",
    "\n",
    "# Create two dictionaries to convert the characters into numerical format and then back to characters.\n",
    "\n",
    "char_to_index = dict((c, i) for i, c in enumerate(characters)) # creating a dictionary in which has the character's index as a value. Enumerate assigns a number to each set\n",
    "index_to_char = dict((i, c) for i, c in enumerate(characters)) # creating a dictionary in which has the number's index as a value. Enumerate assigns a number to each set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "next_characters = [] # scanning a sentence and the result will be the following charcaters. Example: \"How are y...\", the result will be \"ou\"\n",
    "\n",
    "SEQ_length = 200 # sequence length to rely on 200 charcaters to predict the next characters. Too high causes overfiitting while too low causes model to be undertrained.\n",
    "step_size = 3 # how many characters are we going to shift to the next sentence. \n",
    "\n",
    "for i in range(0, len(textfile) - SEQ_length, step_size): # from beginnning of the text. to the end as long as we can read the sequence length, and with step size of 3 charcaters.\n",
    "    sentences.append(textfile[i: i+SEQ_length]) # append to the setences list, part of the text that starts from i up until i plus Sequence Length.\n",
    "    next_characters.append(textfile[i+SEQ_length]) # if the total SEQ_length is 40 we get prediction from character 0 up until 39. And so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((len(sentences), SEQ_length, len(characters)), dtype=bool) # creating a numpy array full of zeros in the shape of the number of sentences, times the sequence length, times the amount of possible characters.\n",
    "\n",
    "y = np.zeros((len(sentences), len(characters)), dtype = bool)  # both x and y are in form of a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(sentences):\n",
    "    for t, character in enumerate(sentence):\n",
    "        x[i, t, char_to_index[character]] = 1\n",
    "    y[i, char_to_index[next_characters[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i> <center> Now we have prepared the Data, time to build the Neural Network for the center part of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3s/step - loss: 3.1102 - val_loss: 2.8866 - learning_rate: 0.0010\n",
      "Epoch 2/4\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2s/step - loss: 2.8566 - val_loss: 2.8820 - learning_rate: 0.0010\n",
      "Epoch 3/4\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2s/step - loss: 2.8921 - val_loss: 2.8534 - learning_rate: 0.0010\n",
      "Epoch 4/4\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2s/step - loss: 2.8521 - val_loss: 2.8541 - learning_rate: 0.0010\n",
      "Epoch 1/4\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2s/step - loss: 2.8839 - val_loss: 2.8514\n",
      "Epoch 2/4\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2s/step - loss: 2.8564 - val_loss: 2.8600\n",
      "Epoch 3/4\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2s/step - loss: 2.8901 - val_loss: 2.8303\n",
      "Epoch 4/4\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2s/step - loss: 2.8655 - val_loss: 2.8389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1e74265d590>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequentialfunc()\n",
    "model.add(LSTMfunc(250, input_shape = (SEQ_length, len(characters)))) # 128 neurons and input shape to put into LSTM layer\n",
    "model.add(Densefunc(len(characters)))\n",
    "model.add(Activationfunc(\"softmax\"))\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer=RMSpropfunc(learning_rate=0.001)) \n",
    "\n",
    "print(x.shape)\n",
    "print(type(x))\n",
    "print(y.shape)\n",
    "print(type(y))\n",
    "\n",
    "model.fit(x, y, batch_size = 10, epochs = 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i> <center> Now to load back the model & Create the text generation function feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"textgenerator.keras\") # Saving model\n",
    "\n",
    "# loaded_model = tf.keras.models.load_model(\"textgenerator.keras\") # Loading model\n",
    "\n",
    "# This first function takes the preds of model, picks one char and depending on the temp will be either conservative or experimental so if we have a high temp we'll choose a char more risky/exp whilst with lower temp a safer pick.\n",
    "\n",
    "def sample(preds, temperature = 1.0): \n",
    "    preds = np.array(preds).astype(\"float64\")\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_text(length, temperature):\n",
    "    start_index = random.randint(0,len(textfile) - SEQ_length - 1)\n",
    "    generated = \"\"\n",
    "    sentence = textfile[start_index: start_index + SEQ_length]\n",
    "    generated += sentence\n",
    "    for i in range(length):\n",
    "        x = np.zeros((1, SEQ_length, len(characters)))\n",
    "        for t, character in enumerate(sentence):\n",
    "            x[0, t, char_to_index[character]] = 1\n",
    "        \n",
    "        predictions = model.predict(x, verbose = 0)[0]\n",
    "        next_index = sample(predictions, temperature)\n",
    "        next_character = index_to_char[next_index]\n",
    "\n",
    "        generated += next_character\n",
    "        sentence = sentence[1:] + next_character\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: [' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Character to index: {' ': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n",
      "Index to character: {0: ' ', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
      "Input shape: (2937, 200, 27)\n",
      "Output shape: (2937, 27)\n"
     ]
    }
   ],
   "source": [
    "# Check the characters and their indices\n",
    "print(\"Characters:\", characters)\n",
    "print(\"Character to index:\", char_to_index)\n",
    "print(\"Index to character:\", index_to_char)\n",
    "\n",
    "# Check the shapes of input and output data\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorflow or keras for building neural networks  opencv for image processing  and the robot operating system  ros  for integrating all the sensors and components  the carla simulator can be used to test                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(500, 0.005))\n",
    "# print(generate_text(400, 0.1))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
